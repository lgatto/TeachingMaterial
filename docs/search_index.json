[
["index.html", "An Introduction to Machine Learning with R Chapter 1 Preface 1.1 Caution 1.2 Installation 1.3 Contact", " An Introduction to Machine Learning with R Laurent Gatto 2017-10-11 Chapter 1 Preface This course material is aimed at people who are already familiar with the R language and syntax, and who would like to get a hands-on introduction to machine learning. 1.1 Caution This material is currently under development and is likely to change in the future. 1.2 Installation A set of packages that are used, either directly or indirectly are provided in the first chapter. A complete session information with all packages used to compile this document is available at the end. The source code for this document is available on GitHub at https://github.com/lgatto/IntroMachineLearningWithR/ A short URL for this book is http://bit.ly/intromlr 1.3 Contact Feel free to contact me for any question or comments, preferably by opening an issue on GitHub. "],
["an-introduction-to-machine-learning-with-r.html", "Chapter 2 An Introduction to Machine Learning with R 2.1 Objectives and pre-requisites 2.2 Why R? 2.3 Overview of machine learning (ML) 2.4 Material and methods", " Chapter 2 An Introduction to Machine Learning with R This introductory workshop on machine learning with R is aimed at participants who are not experts in machine learning (introductory material will be presented as part of the course), but have some familiarity with scripting in general and R in particular. The workshop will offer a hands-on overview of typical machine learning applications in R, including unsupervised (clustering, such as hierarchical and k-means clustering, and dimensionality reduction, such as principal component analysis) and supervised (classification and regression, such as K-nearest neighbour and linear regression) methods. We will also address questions such as model selection using cross-validation. The material has an important hands-on component and readers should have a computer running R 3.4.1 or later. 2.1 Objectives and pre-requisites The course aims at providing an accessible introduction to various machine learning methods and applications in R. The core of the courses focuses on unsupervised and supervised methods. The course contains numerous exercises to provide amble opportunity to apply the newly learnt material. Participants are expected to be familiar with the R syntax and basic plotting functionality. At the end of the course, the participants are anticipated to be able to apply the methods learnt, as well as feel confident enough to explore and apply new methods. 2.2 Why R? R is one of the major languages for data science currently available. It provides excellent visualisation features, which is essential to explore the data before submitting it to any automated learning, as well as assessing the results of the learning algorithm. Many R package for machine learning are available of the shelf and many modern methods in statistical learning are implemented in R as part of their development. Nowadays, there are however other viable alternatives that benefit from similar advantages. If we consider python, for example, the scikit-learn software provides all tools that we will present in this course. 2.3 Overview of machine learning (ML) In supervised learning (SML), the learning algorithm is presented with labelled example inputs, where the labels indicate the desired output. SML itself is composed of classification, where the output is categorical, and regression, where the output is numerical. In unsupervised learning (UML), no labels are provided, and the learning algorithm focuses solely on detecting structure in unlabelled input data. Note that there are also semi-supervised learning approaches that use labelled data to inform unsupervised learning on the unlabelled data to identify and annotate new classes in the dataset (also called novelty detection). Reinforcement learning, the learning algorithm performs a task using feedback from operating in a real of synthetic environment. 2.4 Material and methods 2.4.1 Example data Observations, examples or simply data points along the rows Features or variables along the columns Using the iris data as an example, for UML, we would have 4 features for each unlabelled example. Sepal.Length Sepal.Width Petal.Length Petal.Width 5.1 3.5 1.4 0.2 4.9 3.0 1.4 0.2 4.7 3.2 1.3 0.2 4.6 3.1 1.5 0.2 5.0 3.6 1.4 0.2 5.4 3.9 1.7 0.4 The same dataset used in the context of SML contains an additional column of labels, documenting the outcome or class of each example. Species Sepal.Length Sepal.Width Petal.Length Petal.Width setosa 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 The different datasets that are used throughout the course are collected and briefly described in the short Data chapter. 2.4.2 Packages We will be using, directly or indirectly, the following packages through the chapters: caret ggplot2 mlbench class caTools randomForst impute ranger kernlab rpart rpart.plot "],
["example-datasets.html", "Chapter 3 Example datasets 3.1 Edgar Anderson’s Iris Data 3.2 Motor Trend Car Road Tests 3.3 Sub-cellular localisation 3.4 The diamonds data 3.5 The Sonar data 3.6 Housing Values in Suburbs of Boston 3.7 Customer churn", " Chapter 3 Example datasets 3.1 Edgar Anderson’s Iris Data In R: data(iris) From the iris manual page: This famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. datatable(iris) For more details, see ?iris. 3.2 Motor Trend Car Road Tests In R data(mtcars) From the ?mtcars manual page: The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). datatable(mtcars) For more details, see ?mtcars. 3.3 Sub-cellular localisation The hyperLOPIT2015 data is used to demonstrate t-SNE and its comparison to PCA. These data provide sub-cellular localisation of proteins in Mouse E14TG2a embryonic stem cells, as published in Christoforou et al. (2016). The data comes as an MSnSet object from the Biocpkg(&quot;MSnbase&quot;) package, specifically developed for such quantitative proteomics data. Alternatively, comma-separated files containing a somehow simplified version of the data can also be found here. These data are only used to illustrate some concepts and are not loaded and used directly to avoid installing numerous dependencies. They are available through the Bioconductor project and can be installed with source(&quot;http://www.bioconductor.org/biocLite.R&quot;) biocLite(c(&quot;MSnbsase&quot;, &quot;pRoloc&quot;)) ## software biocLite(&quot;pRolocdata&quot;) ## date 3.4 The diamonds data The diamonds data ships with the ggplot2 package and predict the price (in US dollars) of about 54000 round cut diamonds. In R: library(&quot;ggplot2&quot;) data(diamonds) datatable(diamonds) ## Warning in instance$preRenderHook(instance): It seems your data is too ## big for client-side DataTables. You may consider server-side processing: ## http://rstudio.github.io/DT/server.html See also ?diamonds. 3.5 The Sonar data The Sonar data from the mlbench package can be used to train a classifer to recognise mines from rocks using sonar data. The data is composed to 60 features representing the energy within a particular frequency band. In R: library(&quot;mlbench&quot;) data(Sinar) ## Warning in data(Sinar): data set &#39;Sinar&#39; not found datatable(Sonar) See also ?Sonar. 3.6 Housing Values in Suburbs of Boston The Boston data from the MASS provides the median value of owner-occupied homes (medv) in $1000s as well as 13 other features for 506 homes in Boston. In R: library(&quot;MASS&quot;) data(Boston) datatable(Boston) See also ?Boston. 3.7 Customer churn This data from the C50 package and distributes a training set with 3333 samples and a test set containing 1667 samples of customer attrition. In R: library(&quot;C50&quot;) data(churn) dim(churnTrain) ## [1] 3333 20 dim(churnTest) ## [1] 1667 20 datatable(churnTrain) "],
["unsupervised-learning.html", "Chapter 4 Unsupervised Learning 4.1 Introduction 4.2 k-means clustering 4.3 Hierarchical clustering 4.4 Pre-processing 4.5 Principal component analysis (PCA) 4.6 t-Distributed Stochastic Neighbour Embedding", " Chapter 4 Unsupervised Learning 4.1 Introduction In unsupervised learning (UML), no labels are provided, and the learning algorithm focuses solely on detecting structure in unlabelled input data. One generally differentiates between Clustering, where the goal is to find homogeneous subgroups within the data; the grouping is based on distance between observations. Dimensional reduction, where the goal is to identify patterns in the features of the data. Dimensionality reduction is often used to facilitate visualisation of the data, as well as a pre-processing method before supervised learning. UML presents specific challenges and benefits: there is no single goal in UML there is generally much more unlabelled data available than labelled data. 4.2 k-means clustering The k-means clustering algorithms aims at partitioning n observations into a fixed number of k clusters. The algorithm will find homogeneous clusters. In R, we use stats::kmeans(x, centers = 3, nstart = 10) where x is a numeric data matrix centers is the pre-defined number of clusters the k-means algorithm has a random component and can be repeated nstart times to improve the returned model Challenge: To learn about k-means, let’s use the iris with the sepal and petal length variables only (to facilitate visualisation). Create such a data matrix and name it x Run the k-means algorithm on the newly generated data x, save the results in a new variable cl, and explore its output when printed. The actual results of the algorithms, i.e. the cluster membership can be accessed in the clusters element of the clustering result output. Use it to colour the inferred clusters to generate a figure like shown below. Figure 4.1: k-means algorithm on sepal and petal lengths i &lt;- grep(&quot;Length&quot;, names(iris)) x &lt;- iris[, i] cl &lt;- kmeans(x, 3, nstart = 10) plot(x, col = cl$cluster) 4.2.1 How does k-means work Initialisation: randomly assign class membership init &lt;- sample(3, nrow(x), replace = TRUE) plot(x, col = init) (#fig:kmworks_init)k-mean random intialisation Iteration: Calculate the centre of each subgroup as the average position of all observations is that subgroup. Each observation is then assigned to the group of its nearest centre. It’s also possible to stop the algorithm after a certain number of iterations, or once the centres move less than a certain distance. par(mfrow = c(1, 2)) plot(x, col = init) centres &lt;- sapply(1:3, function(i) colMeans(x[init == i, ], )) centres &lt;- t(centres) points(centres[, 1], centres[, 2], pch = 19, col = 1:3) tmp &lt;- dist(rbind(centres, x)) tmp &lt;- as.matrix(tmp)[, 1:3] ki &lt;- apply(tmp, 1, which.min) ki &lt;- ki[-(1:3)] plot(x, col = ki) points(centres[, 1], centres[, 2], pch = 19, col = 1:3) (#fig:kmworks_iter)k-means iteration: calculate centers (left) and assign new cluster membership (right) Termination: Repeat iteration until no point changes its cluster membership. k-means convergence (credit Wikipedia) 4.2.2 Model selection Due to the random initialisation, one can obtain different clustering results. When k-means is run multiple times, the best outcome, i.e. the one that generates the smallest total within cluster sum of squares (SS), is selected. The total within SS is calculated as: For each cluster results: for each observation, determine the squared euclidean distance from observation to centre of cluster sum all distances Note that this is a local minimum; there is no guarantee to obtain a global minimum. Challenge: Repeat kmeans on our x data multiple times, setting the number of iterations to 1 or greater and check whether you repeatedly obtain the same results. Try the same with random data of identical dimensions. cl1 &lt;- kmeans(x, centers = 3, nstart = 10) cl2 &lt;- kmeans(x, centers = 3, nstart = 10) table(cl1$cluster, cl2$cluster) ## ## 1 2 3 ## 1 0 0 58 ## 2 0 51 0 ## 3 41 0 0 cl1 &lt;- kmeans(x, centers = 3, nstart = 1) cl2 &lt;- kmeans(x, centers = 3, nstart = 1) table(cl1$cluster, cl2$cluster) ## ## 1 2 3 ## 1 51 0 0 ## 2 0 0 58 ## 3 0 41 0 set.seed(42) xr &lt;- matrix(rnorm(prod(dim(x))), ncol = ncol(x)) cl1 &lt;- kmeans(xr, centers = 3, nstart = 1) cl2 &lt;- kmeans(xr, centers = 3, nstart = 1) table(cl1$cluster, cl2$cluster) ## ## 1 2 3 ## 1 46 0 6 ## 2 1 51 0 ## 3 0 1 45 diffres &lt;- cl1$cluster != cl2$cluster par(mfrow = c(1, 2)) plot(xr, col = cl1$cluster, pch = ifelse(diffres, 19, 1)) plot(xr, col = cl2$cluster, pch = ifelse(diffres, 19, 1)) Figure 4.2: Different k-means results on the same (random) data 4.2.3 How to determine the number of clusters Run k-means with k=1, k=2, …, k=n Record total within SS for each value of k. Choose k at the elbow position, as illustrated below. Challenge Calculate the total within sum of squares for k from 1 to 5 for our x test data, and reproduce the figure above. ks &lt;- 1:5 tot_within_ss &lt;- sapply(ks, function(k) { cl &lt;- kmeans(x, k, nstart = 10) cl$tot.withinss }) plot(ks, tot_within_ss, type = &quot;b&quot;) 4.3 Hierarchical clustering 4.3.1 How does hierarchical clustering work Initialisation: Starts by assigning each of the n point its own cluster Iteration Find the two nearest clusters, and join them together, leading to n-1 clusters Continue merging cluster process until all are grouped into a single cluster Termination: All observations are grouped within a single cluster. Figure 4.3: Hierarchical clustering: initialisation (left) and colour-coded results after iteration (right). The results of hierarchical clustering are typically visualised along a dendrogram, where the distance between the clusters is proportional to the branch lengths. Figure 4.4: Visualisation of the hierarchical clustering results on a dendrogram In R: Calculate the distance using dist, typically the Euclidean distance. Hierarchical clustering on this distance matrix using hclust Challenge Apply hierarchical clustering on the iris data and generate a dendrogram using the dedicated plot method. d &lt;- dist(iris[, 1:4]) hcl &lt;- hclust(d) hcl ## ## Call: ## hclust(d = d) ## ## Cluster method : complete ## Distance : euclidean ## Number of objects: 150 plot(hcl) 4.3.2 Defining clusters After producing the hierarchical clustering result, we need to cut the tree (dendrogram) at a specific height to defined the clusters. For example, on our test dataset above, we could decide to cut it at a distance around 1.5, with would produce 2 clusters. Figure 4.5: Cutting the dendrogram at height 1.5. In R we can us the cutree function to cut the tree at a specific height: cutree(hcl, h = 1.5) cut the tree to get a certain number of clusters: cutree(hcl, k = 2) Challenge Cut the iris hierarchical clustering result at a height to obtain 3 clusters by setting h. Cut the iris hierarchical clustering result at a height to obtain 3 clusters by setting directly k, and verify that both provide the same results. plot(hcl) abline(h = 3.9, col = &quot;red&quot;) cutree(hcl, k = 3) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 2 3 2 3 2 3 3 3 3 2 3 2 3 3 2 3 ## [71] 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2 2 2 3 3 3 2 3 3 3 3 3 2 3 3 ## [ reached getOption(&quot;max.print&quot;) -- omitted 50 entries ] cutree(hcl, h = 3.9) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 2 3 2 3 2 3 3 3 3 2 3 2 3 3 2 3 ## [71] 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2 2 2 3 3 3 2 3 3 3 3 3 2 3 3 ## [ reached getOption(&quot;max.print&quot;) -- omitted 50 entries ] identical(cutree(hcl, k = 3), cutree(hcl, h = 3.9)) ## [1] TRUE Challenge Using the same value k = 3, verify if k-means and hierarchical clustering produce the same results on the iris data. Which one, if any, is correct? km &lt;- kmeans(iris[, 1:4], centers = 3, nstart = 10) hcl &lt;- hclust(dist(iris[, 1:4])) table(km$cluster, cutree(hcl, k = 3)) ## ## 1 2 3 ## 1 0 38 0 ## 2 50 0 0 ## 3 0 34 28 par(mfrow = c(1, 2)) plot(iris$Petal.Length, iris$Sepal.Length, col = km$cluster, main = &quot;k-means&quot;) plot(iris$Petal.Length, iris$Sepal.Length, col = cutree(hcl, k = 3), main = &quot;Hierarchical clustering&quot;) ## Checking with the labels provided with the iris data table(iris$Species, km$cluster) ## ## 1 2 3 ## setosa 0 50 0 ## versicolor 2 0 48 ## virginica 36 0 14 table(iris$Species, cutree(hcl, k = 3)) ## ## 1 2 3 ## setosa 50 0 0 ## versicolor 0 23 27 ## virginica 0 49 1 4.4 Pre-processing Many of the machine learning methods that are regularly used are sensitive to difference scales. This applies to unsupervised methods as well as supervised methods, as we will see in the next chapter. A typical way to pre-process the data prior to learning is to scale the data, or apply principal component analysis (next section). Scaling assures that all data columns have mean 0 and standard deviate 1. In R, scaling is done with the scale function. Challenge Using the mtcars data as an example, verify that the variables are of different scales, then scale the data. To observe the effect different scales, compare the hierarchical clusters obtained on the original and scaled data. colMeans(mtcars) ## mpg cyl disp hp drat wt ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 ## qsec vs am gear carb ## 17.848750 0.437500 0.406250 3.687500 2.812500 hcl1 &lt;- hclust(dist(mtcars)) hcl2 &lt;- hclust(dist(scale(mtcars))) par(mfrow = c(1, 2)) plot(hcl1, main = &quot;original data&quot;) plot(hcl2, main = &quot;scaled data&quot;) 4.5 Principal component analysis (PCA) Dimensionality reduction techniques are widely used and versatile techniques that can be used o find structure in features pre-processing for other ML algorithms, and as an aid in visualisation. The basic principle of dimensionality reduction techniques is to transform the data into a new space that summarise properties of the whole data set along a reduced number of dimensions. These are then ideal candidates used to visualise the data along these reduced number of informative dimensions. 4.5.1 How does it work Principal Component Analysis (PCA) is a technique that transforms the original n-dimensional data into a new n-dimensional space. These new dimensions are linear combinations of the original data, i.e. they are composed of proportions of the original variables. Along these new dimensions, called principal components, the data expresses most of its variability along the first PC, then second, … Principal components are orthogonal to each other, i.e. non-correlated. Figure 4.6: Original data (left). PC1 will maximise the variability while minimising the residuals (centre). PC2 is orthogonal to PC1 (right). In R, we can use the prcomp function. Let’s explore PCA on the iris data. While it contains only 4 variables, is already becomes difficult to visualise the 3 groups along all these dimensions. pairs(iris[, -5], col = iris[, 5], pch = 19) Let’s use PCA to reduce the dimension. irispca &lt;- prcomp(iris[, -5]) summary(irispca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 2.0563 0.49262 0.2797 0.15439 ## Proportion of Variance 0.9246 0.05307 0.0171 0.00521 ## Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 A summary of the prcomp output shows that along PC1 along, we are able to retain over 92% of the total variability in the data. Figure 4.7: Iris data along PC1. 4.5.2 Visualisation A biplot features all original points re-mapped (rotated) along the first two PCs as well as the original features as vectors along the same PCs. Feature vectors that are in the same direction in PC space are also correlated in the original data space. biplot(irispca) One important piece of information when using PCA is the proportion of variance explained along the PCs, in particular when dealing with high dimensional data, as PC1 and PC2 (that are generally used for visualisation), might only account for an insufficient proportion of variance to be relevant on their own. In the code chunk below, I extract the standard deviations from the PCA result to calculate the variances, then obtain the percentage of and cumulative variance along the PCs. var &lt;- irispca$sdev^2 (pve &lt;- var/sum(var)) ## [1] 0.924618723 0.053066483 0.017102610 0.005212184 cumsum(pve) ## [1] 0.9246187 0.9776852 0.9947878 1.0000000 Challenge Repeat the PCA analysis on the iris dataset above, reproducing the biplot and preparing a barplot of the percentage of variance explained by each PC. It is often useful to produce custom figures using the data coordinates in PCA space, which can be accessed as x in the prcomp object. Reproduce the PCA plots below, along PC1 and PC2 and PC3 and PC4 respectively. par(mfrow = c(1, 2)) plot(irispca$x[, 1:2], col = iris$Species) plot(irispca$x[, 3:4], col = iris$Species) 4.5.3 Data pre-processing We haven’t looked at other prcomp parameters, other that the first one, x. There are two other ones that are or importance, in particular in the light of the section on pre-processing above, which are center and scale.. The former is set to TRUE by default, while the second one is set the `FALSE. Challenge Repeat the analysis comparing the need for scaling on the mtcars dataset, but using PCA instead of hierarchical clustering. When comparing the two. par(mfrow = c(1, 2)) biplot(prcomp(mtcars, scale = FALSE), main = &quot;No scaling&quot;) ## 1 biplot(prcomp(mtcars, scale = TRUE), main = &quot;With scaling&quot;) ## 2 Without scaling, disp and hp are the features with the highest loadings along PC1 and 2 (all others are negligible), which are also those with the highest units of measurement. Scaling removes this effect. 4.5.4 Final comments on PCA Real datasets often come with missing values. In R, these should be encoded using NA. Unfortunately, PCA cannot deal with missing values, and observations containing NA values will be dropped automatically. This is a viable solution only when the proportion of missing values is low. It is also possible to impute missing values. This is described in greater details in the Data pre-processing section in the supervised machine learning chapter. Finally, we should be careful when using categorical data in any of the unsupervised methods described above. Categories are generally represented as factors, which are encoded as integer levels, and might give the impression that a distance between levels is a relevant measure (which it is not, unless the factors are ordered). In such situations, categorical data can be dropped, or it is possible to encode categories as binary dummy variables. For example, if we have 3 categories, say A, B and C, we would create two dummy variables to encode the categories as: A: 1 and 0 B: 0 and 1 C: 0 and 0 (neither A nor B) so that the distance between each category are approximately equal to 1. 4.6 t-Distributed Stochastic Neighbour Embedding t-Distributed Stochastic Neighbour Embedding (t-SNE) is a non-linear dimensionality reduction technique, i.e. that different regions of the data space will be subjected to different transformations. t-SNE will compress small distances, thus bringing close neighbours together, and will ignore large distances. It is particularly well suited for very high dimensional data. In R, we can use the Rtsne function from the Rtsne. Before, we however need to remove any duplicated entries in the dataset. library(&quot;Rtsne&quot;) uiris &lt;- unique(iris[, 1:5]) iristsne &lt;- Rtsne(uiris[, 1:4]) plot(iristsne$Y, col = uiris$Species) As with PCA, the data can be scaled and centred prior the running t-SNE (see the pca_center and pca_scale arguments). The algorithm is stochastic, and will produce different results at each repetition. 4.6.1 Parameter tuning t-SNE (as well as many other methods, in particular classification algorithms) has two important parameters that can substantially influence the clustering of the data Perplexity: balances global and local aspects of the data. Iterations: number of iterations before the clustering is stopped. It is important to adapt these for different data. The figure below shows a 5032 by 20 dataset that represent protein sub-cellular localisation. Effect of different perplexity and iterations when running t-SNE As a comparison, below are the same data with PCA (left) and t-SNE (right). PCA and t-SNE on hyperLOPIT "],
["supervised-learning.html", "Chapter 5 Supervised Learning 5.1 Introduction 5.2 Preview 5.3 Model performance 5.4 Classification performance 5.5 Random forest 5.6 Data pre-processing 5.7 Scaling and scaling 5.8 Model selection", " Chapter 5 Supervised Learning 5.1 Introduction In supervised learning (SML), the learning algorithm is presented with labelled example inputs, where the labels indicate the desired output. SML itself is composed of classification, where the output is qualitative, and regression, where the output is quantitative. When two sets of labels, or classes, are available, one speaks of binary classification. A classical example thereof is labelling an email as spam or not spam. When more classes are to be learnt, one speaks of a multi-class problem, such as annotation a new Iris example as being from the setosa, versicolor or virginica species. In these cases, the output is a single label (of one of the anticipated classes). If multiple labels may be assigned to each examples, one speaks of multi-label classification. 5.2 Preview To start this chapter, let’s use a simple, but useful classification algorithm, K nearest neighbours (kNN) to classify the iris flowers. We will use the knn function from the class package. K nearest neighbours works by directly measuring the (euclidean) distance between observations and infer the class of unlabelled data from the class of its nearest neighbours. In the figure below, the unlabelled instances 1 and 2 will be assigned classes c1 (blue) and c2 (red) as their closest neighbours are red and blue, respectively. Figure 5.1: Schematic illustrating the k nearest neighbors algorithm. Typically in machine learning, there are two clear steps, where on first trains a model and then only uses the model to predict new outputs (class labels in this case). In the kNN, these two steps are combined into a single function call to knn. Let draw a set of 50 random iris observations to train the model and predict the species of another set of 50 randomly chosen flowers. The knn function takes the training data, the new data (to be inferred) and the labels of the training data, and returns (by default) the predicted class. set.seed(12L) tr &lt;- sample(150, 50) nw &lt;- sample(150, 50) library(&quot;class&quot;) knnres &lt;- knn(iris[tr, -5], iris[nw, -5], iris$Species[tr]) head(knnres) ## [1] versicolor versicolor versicolor setosa versicolor virginica ## Levels: setosa versicolor virginica We can now compare the observed kNN-predicted class and the expected known outcome and calculate the overall accuracy of our model. table(knnres, iris$Species[nw]) ## ## knnres setosa versicolor virginica ## setosa 18 0 0 ## versicolor 0 18 1 ## virginica 0 1 12 mean(knnres == iris$Species[nw]) ## [1] 0.96 We have omitted and important argument from knn, which is the parameter k of the classifier. This value k defines how many nearest neighbours will be considered to assign a class to a new unlabelled observation. From the arguments of the function, args(knn) ## function (train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE) ## NULL we see that the default value is 1. But is this a good value? Wouldn’t we prefer to look at more neighbours and infer the new class using a vote based on more labels? Challenge Repeat the kNN classification above by using another value of k, and compare the accuracy of this new model to the one above. Make sure to use the same tr and nw training and new data to avoid any biases in the comparison. knnres5 &lt;- knn(iris[tr, -5], iris[nw, -5], iris$Species[tr], k = 5) mean(knnres5 == iris$Species[nw]) ## [1] 0.94 table(knnres5, knnres) ## knnres ## knnres5 setosa versicolor virginica ## setosa 18 0 0 ## versicolor 0 18 0 ## virginica 0 1 13 Challenge Rerun the kNN classifier with a value of k &gt; 1, and specify prob = TRUE to obtain the proportion of the votes for the winning class. knnres5prob &lt;- knn(iris[tr, -5], iris[nw, -5], iris$Species[tr], k = 5, prob = TRUE) table(attr(knnres5prob, &quot;prob&quot;)) ## ## 0.6 0.8 1 ## 4 1 45 This introductory example leads to two important and related questions that we need to consider: How can we do a good job in training and testing data? In the example above, we choose random training and new data, and needed to make sure to keep these identical for out subsequent comparison. How can we estimate our model parameters so as to obtain good classification accuracy? 5.3 Model performance 5.3.1 In-sample and out-of-sample error In supervised machine learning, we have a desired output and thus know precisely what is to be computed. It thus becomes possible to directly evaluate a model using a quantifiable and object metric. For regression, we will use the root mean squared error (RMSE), which is what linear regression (lm in R) seeks to minimise. For classification, we will use model prediction accuracy. Typically, we won’t want to calculate any of these metrics using observations that were also used to calculate the model. This approach, called in-sample error lead to optimistic assessment of our model. Indeed, the model has already seen these data upon construction, and is does considered optimised the these observations in particular; it is said to over-fit the data. We prefer to calculate an out-of-sample error, on new data, to gain a better idea of how to model performs on unseen data, and estimate how well the model generalises. In this course, we will focus on the caret package for Classification And REgression Training (see also https://topepo.github.io/caret/index.html). It provides a common and consistent interface to many, often repetitive, tasks in supervised learning. library(&quot;caret&quot;) The code chunk below uses the lm function to model the price of round cut diamonds and then predicts the price of these very same diamonds with the predict function. data(diamonds) model &lt;- lm(price ~ ., diamonds) p &lt;- predict(model, diamonds) Challenge Calculate the root mean squares error for the prediction above ## Error on prediction error &lt;- p - diamonds$price rmse_in &lt;- sqrt(mean(error^2)) ## in-sample RMSE rmse_in ## [1] 1129.843 Let’s now repeat the exercise above, but by calculating the out-of-sample RMSE. We are prepare a 80/20 split of the data and use 80% to fit our model predict the target variable (this is called the training data), the price, on the 20% unseen data (the testing data). Challenge Let’s create a random 80/20 split to define the test and train subsets. Train a regression model on the training data. Test the model on the testing data. Calculating the out-of-sample RMSE. set.seed(42) ntest &lt;- nrow(diamonds) * 0.80 test &lt;- sample(nrow(diamonds), ntest) model &lt;- lm(price ~ ., data = diamonds[test, ]) p &lt;- predict(model, diamonds[-test, ]) error &lt;- p - diamonds$price[-test] rmse_out &lt;- sqrt(mean(error^2)) ## out-of-sample RMSE rmse_out ## [1] 1136.596 The values for the out-of-sample RMSE will vary depending on the what exact split was used. The diamonds is a rather extensive data, and thus even when building out model using a subset of the available data (80% above), we manage to generate a model with a low RMSE, and possibly lower than the in-sample error. When dealing with datasets of smaller sizes, however, the presence of a single outlier in the train and test data split can substantially influence the model and the RMSE. We can’t rely on such an approach an need a more robust one, where, we can generate and use multiple, different train/test sets to sample a set of RMSEs, leading to a better estimate of the out-of-sample RMSE. 5.3.2 Cross-validation Instead of doing a single training/testing split, we can systematise this process, produce multiple, different out-of-sample train/test splits, that will lead to a better estimate of the out-of-sample RMSE. The figure below illustrates the cross validation procedure, creating 3 folds. One would typically do a 10-fold cross validation (if the size of the data permits it). We split the data into 3 random and complementary folds, so that each data point appears exactly once in each fold. This leads to a total test set size that is identical to the size as the full dataset but is composed of out-of-sample predictions. Schematic of 3-fold cross validation producing three training (blue) and testing (white) splits. After cross-validation, all models used within each fold are discarded, and a new model is build using the whole dataset, with the best model parameter(s), i.e those that generalised over all folds. This makes cross-validation quite time consuming, as it takes x+1 (where x in the number of cross-validation folds) times as long as fitting a single model, but is essential. Note that it is important to maintain the class proportions within the different folds, i.e. respect the proportion of the different classes in the original data. This is also taken care when using the caret package. The procedure of creating folds and training the models is handled by the train function in caret. Below, we apply it to the diamond price example that we used when introducing the model performance. We start by setting a random to be able to reproduce the example. We specify the method (the learning algorithm) we want to use. Here, we use &quot;lm&quot;, but, as we will see later, there are many others to choose from1. We then set the out-of-sample training procedure to 10-fold cross validation (method = &quot;cv&quot; and number = 10). To simplify the output in the material for better readability, we set the verbosity flag to FALSE, but it is useful to set it to TRUE in interactive mode. set.seed(42) model &lt;- train(price ~ ., diamonds, method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10, verboseIter = FALSE)) model ## Linear Regression ## ## 53940 samples ## 9 predictors ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 48545, 48547, 48546, 48546, 48545, 48546, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1131.504 0.9195556 740.6424 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE Once we have trained our model, we can directly use object of train as input to the predict method: p &lt;- predict(model, diamonds) error &lt;- p - diamonds$price rmse_xval &lt;- sqrt(mean(error^2)) ## xval RMSE rmse_xval ## [1] 1129.843 Challenge Train a linear model using 10-fold cross-validation and then use it to predict the median value of owner-occupied homes in Boston from the Boston dataset as described above. Then calculate the RMSE. library(&quot;MASS&quot;) data(Boston) model &lt;- train(medv ~ ., Boston, method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10)) model ## Linear Regression ## ## 506 samples ## 13 predictors ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 455, 456, 454, 454, 455, 454, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 4.770438 0.7379312 3.38097 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE p &lt;- predict(model, Boston) sqrt(mean(p - Boston$medv)^2) ## [1] 5.851927e-14 5.4 Classification performance Above, we have used the RMSE to assess the performance of our regression model. When using a classification algorithm, we want to assess its accuracy to do so. 5.4.1 Confusion matrix Instead of calculating an error between predicted value and known value, in classification we will directly compare of the predicted class matches the known label. To do so, rather than calculating the mean accuracy as we did above, in the introductory kNN example, we can calculate a confusion matrix. A confusion matrix to contrast predictions to actual results. Correct results are true positives (TP) and true negatives that are found along the diagonal. All other cells indicate false results, i.e false negatives (FN) and *false positives8 (FP). Reference Yes Referenc No Predicted Yes TP FP Predicted No FN TN The values that populate this table will depend on a the cutoff that we set to define whether the classifier should predict Yes or No. Intuitively, we might want to use 0.5 as a threshold, and assign every result with a probability &gt; 0.5 to Yes and No otherwise. Let’s experiment with this using the Sonar dataset, and see if we can differentiate mines from rocks using a logistic classification model use the glm function from the stats package. library(&quot;mlbench&quot;) data(Sonar) ## 60/40 split tr &lt;- sample(nrow(Sonar), round(nrow(Sonar) * 0.6)) train &lt;- Sonar[tr, ] test &lt;- Sonar[-tr, ] model &lt;- glm(Class ~ ., data = train, family = &quot;binomial&quot;) p &lt;- predict(model, test, type = &quot;response&quot;) summary(p) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.7265 0.5079 1.0000 1.0000 cl &lt;- ifelse(p &gt; 0.5, &quot;M&quot;, &quot;R&quot;) table(cl, test$Class) ## ## cl M R ## M 16 26 ## R 29 12 The caret package offers it’s own, more informative function to calculate a confusion matrix: confusionMatrix(cl, test$Class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction M R ## M 16 26 ## R 29 12 ## ## Accuracy : 0.3373 ## 95% CI : (0.2372, 0.4495) ## No Information Rate : 0.5422 ## P-Value [Acc &gt; NIR] : 0.9999 ## ## Kappa : -0.3266 ## Mcnemar&#39;s Test P-Value : 0.7874 ## ## Sensitivity : 0.3556 ## Specificity : 0.3158 ## Pos Pred Value : 0.3810 ## Neg Pred Value : 0.2927 ## Prevalence : 0.5422 ## Detection Rate : 0.1928 ## Detection Prevalence : 0.5060 ## Balanced Accuracy : 0.3357 ## ## &#39;Positive&#39; Class : M ## We get, among others the accuracy: (TP + TN)/(TP + TN + FP + FN) the sensitivity (recall, TP rate): TP/(TP + FN) the specificity: TN/(TN + FP) positive predictive value (precision): TP/(TP + FP) negative predictive value: TN/(TN + FN) FP rate (fall-out): FP/(FP + TN) Challenge Compare the model accuracy (or any other metric) using thresholds of 0.1 and 0.9. confusionMatrix(ifelse(p &gt; 0.9, &quot;M&quot;, &quot;R&quot;), test$Class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction M R ## M 16 25 ## R 29 13 ## ## Accuracy : 0.3494 ## 95% CI : (0.248, 0.4619) ## No Information Rate : 0.5422 ## P-Value [Acc &gt; NIR] : 0.9999 ## ## Kappa : -0.2999 ## Mcnemar&#39;s Test P-Value : 0.6831 ## ## Sensitivity : 0.3556 ## Specificity : 0.3421 ## Pos Pred Value : 0.3902 ## Neg Pred Value : 0.3095 ## Prevalence : 0.5422 ## Detection Rate : 0.1928 ## Detection Prevalence : 0.4940 ## Balanced Accuracy : 0.3488 ## ## &#39;Positive&#39; Class : M ## confusionMatrix(ifelse(p &gt; 0.1, &quot;M&quot;, &quot;R&quot;), test$Class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction M R ## M 16 28 ## R 29 10 ## ## Accuracy : 0.3133 ## 95% CI : (0.2159, 0.4244) ## No Information Rate : 0.5422 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : -0.3805 ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.3556 ## Specificity : 0.2632 ## Pos Pred Value : 0.3636 ## Neg Pred Value : 0.2564 ## Prevalence : 0.5422 ## Detection Rate : 0.1928 ## Detection Prevalence : 0.5301 ## Balanced Accuracy : 0.3094 ## ## &#39;Positive&#39; Class : M ## 5.4.2 Receiver operating characteristic (ROC) curve There is no reason to use 0.5 as a threshold. One could use a low threshold to catch more mines with less certainty or or higher threshold to catch fewer mines with more certainty. This illustrates the need to adequately balance TP and FP rates. We need to have a way to do a cost-benefit analysis, and the solution will often depend on the question/problem. One solution would be to try with different classification thresholds. Instead of inspecting numerous confusion matrices, it is possible to automate the calculation of the TP and FP rates at each threshold and visualise all results along a ROC curve. This can be done with the colAUC function from the caTools package: caTools::colAUC(p, test[[&quot;Class&quot;]], plotROC = TRUE) ## [,1] ## M vs. R 0.6909357 x: FP rate (1 - specificity) y: TP rate (sensitivity) each point along the curve represents a confusion matrix for a given threshold In addition, the colAUC function returns the area under the curve (AUC) model accuracy metric. This is single number metric, summarising the model performance along all possible thresholds: an AUC of 0.5 corresponds to a random model values &gt; 0.5 do better than a random guess a value 1 represents a perfect model a value 1 represents a model that is always wrong 5.4.3 AUC in caret When using caret’s trainControl function to train a model, we can set it so that it computes the ROC and AUC properties for us. ## Create trainControl object: myControl myControl &lt;- trainControl( method = &quot;cv&quot;, ## cross validation number = 10, ## 10-fold summaryFunction = twoClassSummary, ## NEW classProbs = TRUE, # IMPORTANT verboseIter = FALSE ) ## Train glm with custom trainControl: model model &lt;- train(Class ~ ., Sonar, method = &quot;glm&quot;, ## to use glm&#39;s logistic regression trControl = myControl) ## Print model to console print(model) ## Generalized Linear Model ## ## 208 samples ## 60 predictors ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 187, 187, 187, 187, 188, 188, ... ## Resampling results: ## ## ROC Sens Spec ## 0.7134091 0.7462121 0.67 Challenge Define a train object that uses the AUC and 10-fold cross validation to classify the Sonar data using a logistic regression, as demonstrated above. 5.5 Random forest Random forest models are accurate and non-linear models and robust to over-fitting and hence quite popular. They however require hyperparameters to be tuned manually, like the value k in the example above. Building random forest starts by generating a high number of individual decision trees. A single decision tree isn’t very accurate, but many different trees built using different inputs (with bootstrapped inputs, features and observations) enable to explore a broad search space and, once combined, produce accurate models, a technique called bootstrap aggregation or bagging. 5.5.1 Decision trees A great advantage of decision trees is that they make a complex decision simpler by breaking it down into smaller, simpler decisions using divide-and-conquer strategy. They basically identify a set of if-else conditions that split data according to the value if the features. library(&quot;rpart&quot;) ## recursive partitioning m &lt;- rpart(Class ~ ., data = Sonar, method = &quot;class&quot;) library(&quot;rpart.plot&quot;) rpart.plot(m) Figure 5.2: Descision tree with its if-else conditions p &lt;- predict(m, Sonar, type = &quot;class&quot;) table(p, Sonar$Class) ## ## p M R ## M 95 10 ## R 16 87 Decision trees choose splits based on most homogeneous partitions, and lead to smaller and more homogeneous partitions over their iterations. An issue with single decision trees is that they can grow, and become large and complex with many branches, with corresponds to over-fitting. Over-fitting models noise, rather than general patterns in the data, focusing on subtle patterns (outliers) that won’t generalise. To avoid over-fitting, individual decision trees are pruned. Pruning can happen as a pre-condition when growing the tree, or afterwards, by pruning a large tree. Pre-pruning: stop growing process, i.e stops divide-and-conquer after a certain number of iterations (grows tree at certain predefined level), or requires a minimum number of observations in each mode to allow splitting. Post-pruning: grow a large and complex tree, and reduce its size; nodes and branches that have a negligible effect on the classification accuracy are removed. 5.5.2 Training a random forest Let’s return to random forests and train a model using the train infrastructure from caret: set.seed(12) model &lt;- train(Class ~ ., data = Sonar, method = &quot;ranger&quot;) print(model) ## Random Forest ## ## 208 samples ## 60 predictors ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 208, 208, 208, 208, 208, 208, ... ## Resampling results across tuning parameters: ## ## mtry splitrule Accuracy Kappa ## 2 gini 0.8221049 0.6417590 ## 2 extratrees 0.8346434 0.6669013 ## 31 gini 0.7884549 0.5747543 ## 31 extratrees 0.8396622 0.6774340 ## 60 gini 0.7776562 0.5524942 ## 60 extratrees 0.8359736 0.6694219 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were mtry = 31 and splitrule ## = extratrees. plot(model) The main hyperparameters is mtry, i.e. the number of randomly selected variables used at each split. 2 variables produce random models, while 100s of variables tend to be less random, but risk over-fitting. caret automate the tuning of the hyperparameter using a grid search, which can be parametrised by setting tuneLength (that sets the number of hyperparameter values to test) or directly defining the tuneGrid (the hyperparameter values), which requires knowledge of the model. model &lt;- train(Class ~ ., data = Sonar, method = &quot;ranger&quot;, tuneLength = 5) set.seed(42) myGrid &lt;- expand.grid(mtry = c(5, 10, 20, 40, 60), splitrule = c(&quot;gini&quot;, &quot;extratrees&quot;)) model &lt;- train(Class ~ ., data = Sonar, method = &quot;ranger&quot;, tuneGrid = myGrid, trControl = trainControl(method = &quot;cv&quot;, number = 5, verboseIter = FALSE)) print(model) ## Random Forest ## ## 208 samples ## 60 predictors ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 167, 167, 167, 166, 165 ## Resampling results across tuning parameters: ## ## mtry splitrule Accuracy Kappa ## 5 gini 0.8228209 0.6388558 ## 5 extratrees 0.8415417 0.6764071 ## 10 gini 0.8175999 0.6291739 ## 10 extratrees 0.8512924 0.6968188 ## 20 gini 0.8030819 0.5994688 ## 20 extratrees 0.8509548 0.6969166 ## 40 gini 0.7838020 0.5612192 ## 40 extratrees 0.8414256 0.6782512 ## 60 gini 0.7692840 0.5310516 ## 60 extratrees 0.8460768 0.6880922 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were mtry = 10 and splitrule ## = extratrees. plot(model) Challenge Experiment with training a random forest model as described above, by using 5-fold cross validation, and setting a tuneLength of 5. set.seed(42) model &lt;- train(Class ~ ., data = Sonar, method = &quot;ranger&quot;, tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, number = 5, verboseIter = FALSE)) plot(model) 5.6 Data pre-processing 5.6.1 Missing values Real datasets often come with missing values. In R, these should be encoded using NA. There are basically two approaches to deal with such cases. Drop the observations with missing values, or, if one feature contains a very high proportion of NAs, drop the feature altogether. These approaches are only applicable when the proportion of missing values is relatively small. Otherwise, it could lead to loosing too much data. Impute missing values. Data imputation can however have critical consequences depending on the proportion of missing values and their nature. From a statistical point of view, missing values are classified as missing completely at random (MCAR), missing at random (MAR) or missing not at random (MNAR), and the type of the missing values will influence the efficiency of the imputation method. The figure below shows how different imputation methods perform depending on the proportion and nature of missing values (from Lazar et al., on quantitative proteomics data). Normalised RMSE (RMSE-observation standard deviation ration) describing the effect of different imputation methods depending on the nature and proportion of the missing values: kNN (a), SVDimpute (b), MLE (c), MinDet (d), and MinProb (e). Let’s start by simulating a dataset containing missing values using the mtcars dataset. Below, we will want to predict the mpg variable using cyl, disp, and hp, with the latter containing 10 missing values. data(mtcars) mtcars[sample(nrow(mtcars), 10), &quot;hp&quot;] &lt;- NA Y &lt;- mtcars$mpg ## target variable X &lt;- mtcars[, 2:4] ## predictors If we now wanted to train a model (using the non-formula interface): try(train(X, Y)) ## note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 . ## ## Something is wrong; all the RMSE metric values are missing: ## RMSE Rsquared MAE ## Min. : NA Min. : NA Min. : NA ## 1st Qu.: NA 1st Qu.: NA 1st Qu.: NA ## Median : NA Median : NA Median : NA ## Mean :NaN Mean :NaN Mean :NaN ## 3rd Qu.: NA 3rd Qu.: NA 3rd Qu.: NA ## Max. : NA Max. : NA Max. : NA ## NA&#39;s :2 NA&#39;s :2 NA&#39;s :2 (Note that the occurrence of the error will depend on the model chosen.) We could perform imputation manually, but caret provides a whole range of pre-processing methods, including imputation methods, that can directly be passed when training the model. 5.6.2 Median imputation Imputation using median of features. This methods works well if the data are missing at random. train(X, Y, preProcess = &quot;medianImpute&quot;) ## note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 . ## Random Forest ## ## 32 samples ## 3 predictors ## ## Pre-processing: median imputation (3) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 32, 32, 32, 32, 32, 32, ... ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 2 2.882856 0.808562 2.389234 ## 3 2.888400 0.806260 2.373986 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 2. Imputing using caret also allows to optimise the imputation based on the cross validation splits, as train will do median imputation inside each fold. 5.6.3 KNN imputation If there is a systematic bias in the missing values, then median imputation is known to produce incorrect results. kNN imputation will impute missing values using on other, similar non-missing rows. The default value is 5. train(X, Y, preProcess = &quot;knnImpute&quot;) ## note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 . ## Random Forest ## ## 32 samples ## 3 predictors ## ## Pre-processing: nearest neighbor imputation (3), centered (3), scaled (3) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 32, 32, 32, 32, 32, 32, ... ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 2 2.868534 0.7995565 2.309105 ## 3 2.871339 0.7907874 2.276478 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 2. 5.7 Scaling and scaling We have seen in the Unsupervised learning chapter how data at different scales can substantially disrupt a learning algorithm. Scaling (division by the standard deviation) and centring (subtraction of the mean) can also be applied directly during model training by setting. Note that they are set to be applied by default prior to training. train(X, Y, preProcess = &quot;scale&quot;) train(X, Y, preProcess = &quot;center&quot;) As we have discussed in the section about Principal component analysis, PCA can be used as pre-processing method, generating a set of high-variance and perpendicular predictors, preventing collinearity. train(X, Y, preProcess = &quot;pca&quot;) 5.7.1 Multiple pre-processing methods It is possible to chain multiple processing methods: imputation, center, scale, pca. train(X, Y, preProcess = c(&quot;knnImpute&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;)) ## note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 . ## Random Forest ## ## 32 samples ## 3 predictors ## ## Pre-processing: nearest neighbor imputation (3), centered (3), scaled ## (3), principal component signal extraction (3) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 32, 32, 32, 32, 32, 32, ... ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 2 3.294041 0.7421489 2.703088 ## 3 3.273033 0.7436632 2.696803 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 3. The pre-processing methods above represent a classical order or operations, starting with data imputation to remove missing values, then centring and scaling, prior to PCA. For further details, see ?preProcess. 5.8 Model selection In this final section, we are going to compare different predictive models and chose the best one using the tools presented in the previous sections. To to so, we are going to first create a set of common training controller object with the same train/test folds and model evaluation metrics that we will re-use. This is important to guarantee fair comparison between the different models. For this section, we are going to use the churn data. Below, we see that about 15% of the customers churn. It is important to maintain this proportion in all the folds. library(&quot;C50&quot;) data(churn) table(churnTrain$churn)/nrow(churnTrain) ## ## yes no ## 0.1449145 0.8550855 Previously, when creating a train control object, we specified the method as &quot;cv&quot; and the number of folds. Now, as we want the same folds to be re-used over multiple model training rounds, we are going to pass the train/test splits directly. These splits are created with the createFolds function, which creates a list (here of length 5) containing the element indices for each fold. myFolds &lt;- createFolds(churnTrain$churn, k = 5) str(myFolds) ## List of 5 ## $ Fold1: int [1:666] 1 4 14 16 23 28 29 30 34 35 ... ## $ Fold2: int [1:667] 6 12 17 32 33 41 49 51 58 65 ... ## $ Fold3: int [1:667] 8 15 18 26 37 43 47 50 52 54 ... ## $ Fold4: int [1:666] 2 7 11 13 22 24 25 31 39 40 ... ## $ Fold5: int [1:667] 3 5 9 10 19 20 21 27 38 48 ... Challenge Verify that the folds maintain the proportion of yes/no results. sapply(myFolds, function(i) { table(churnTrain$churn[i])/length(i) }) ## Fold1 Fold2 Fold3 Fold4 Fold5 ## yes 0.1441441 0.1454273 0.1454273 0.1441441 0.1454273 ## no 0.8558559 0.8545727 0.8545727 0.8558559 0.8545727 We can now a train control object to be reused consistently for different model trainings. myControl &lt;- trainControl( summaryFunction = twoClassSummary, classProb = TRUE, verboseIter = FALSE, savePredictions = TRUE, index = myFolds ) NB Some of the model training below will take some time to run, depending on the tuning parameter settings. 5.8.1 glmnet model The glmnet is a liner model with build-in variable selection and coefficient regularisation. glm_model &lt;- train(churn ~ ., churnTrain, metric = &quot;ROC&quot;, method = &quot;glmnet&quot;, tuneGrid = expand.grid( alpha = 0:1, lambda = 0:10/10), trControl = myControl) print(glm_model) ## glmnet ## ## 3333 samples ## 19 predictors ## 2 classes: &#39;yes&#39;, &#39;no&#39; ## ## No pre-processing ## Resampling: Bootstrapped (5 reps) ## Summary of sample sizes: 666, 667, 667, 666, 667 ## Resampling results across tuning parameters: ## ## alpha lambda ROC Sens Spec ## 0 0.0 0.7571208 0.249493246 0.9563158 ## 0 0.1 0.7724348 0.080234566 0.9921053 ## 0 0.2 0.7734827 0.017605870 0.9986842 ## 0 0.3 0.7732454 0.003107469 0.9999123 ## 0 0.4 0.7727991 0.001036269 1.0000000 ## 0 0.5 0.7724129 0.000000000 1.0000000 ## 0 0.6 0.7721133 0.000000000 1.0000000 ## 0 0.7 0.7718314 0.000000000 1.0000000 ## 0 0.8 0.7715527 0.000000000 1.0000000 ## 0 0.9 0.7713204 0.000000000 1.0000000 ## 0 1.0 0.7711282 0.000000000 1.0000000 ## 1 0.0 0.7305374 0.292971041 0.9410526 ## 1 0.1 0.5210535 0.000000000 1.0000000 ## 1 0.2 0.5000000 0.000000000 1.0000000 ## 1 0.3 0.5000000 0.000000000 1.0000000 ## 1 0.4 0.5000000 0.000000000 1.0000000 ## 1 0.5 0.5000000 0.000000000 1.0000000 ## 1 0.6 0.5000000 0.000000000 1.0000000 ## 1 0.7 0.5000000 0.000000000 1.0000000 ## 1 0.8 0.5000000 0.000000000 1.0000000 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were alpha = 0 and lambda = 0.2. plot(glm_model) 5.8.2 random forest model Challenge Apply a random forest model, making sure you reuse the same train control object. rf_model &lt;- train(churn ~ ., churnTrain, metric = &quot;ROC&quot;, method = &quot;ranger&quot;, tuneGrid = expand.grid( mtry = c(2, 5, 10, 19), splitrule = c(&quot;gini&quot;, &quot;extratrees&quot;)), trControl = myControl) print(rf_model) ## Random Forest ## ## 3333 samples ## 19 predictors ## 2 classes: &#39;yes&#39;, &#39;no&#39; ## ## No pre-processing ## Resampling: Bootstrapped (5 reps) ## Summary of sample sizes: 666, 667, 667, 666, 667 ## Resampling results across tuning parameters: ## ## mtry splitrule ROC Sens Spec ## 2 gini 0.8685414 0.01553735 1.0000000 ## 2 extratrees 0.8308926 0.00000000 1.0000000 ## 5 gini 0.8888916 0.18223213 0.9975439 ## 5 extratrees 0.8664525 0.03573121 0.9998246 ## 10 gini 0.8962862 0.35252306 0.9923684 ## 10 extratrees 0.8826687 0.14806202 0.9990351 ## 19 gini 0.9022172 0.57663038 0.9857018 ## 19 extratrees 0.8922735 0.32660160 0.9943860 ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were mtry = 19 and splitrule = gini. plot(rf_model) 5.8.3 kNN model Challenge Apply a kNN model, making sure you reuse the same train control object. knn_model &lt;- train(churn ~ ., churnTrain, metric = &quot;ROC&quot;, method = &quot;knn&quot;, tuneLength = 20, trControl = myControl) print(knn_model) ## k-Nearest Neighbors ## ## 3333 samples ## 19 predictors ## 2 classes: &#39;yes&#39;, &#39;no&#39; ## ## No pre-processing ## Resampling: Bootstrapped (5 reps) ## Summary of sample sizes: 666, 667, 667, 666, 667 ## Resampling results across tuning parameters: ## ## k ROC Sens Spec ## 5 0.6717223 0.21067063 0.9808772 ## 7 0.6861181 0.19618428 0.9880702 ## 9 0.6917436 0.18272750 0.9906140 ## 11 0.6954371 0.16357928 0.9922807 ## 13 0.6991182 0.14650493 0.9935088 ## 15 0.7015124 0.12994604 0.9942982 ## 17 0.7044329 0.12321163 0.9957018 ## 19 0.7050949 0.11492951 0.9959649 ## 21 0.7081416 0.10457485 0.9964912 ## 23 0.7099561 0.09422420 0.9969298 ## 25 0.7113652 0.08698237 0.9972807 ## 27 0.7134698 0.07973384 0.9978070 ## 29 0.7129408 0.06938185 0.9978070 ## 31 0.7114156 0.06213466 0.9982456 ## 33 0.7123250 0.05954533 0.9984211 ## 35 0.7134920 0.05540694 0.9986842 ## 37 0.7150380 0.04867789 0.9988596 ## 39 0.7140439 0.04350056 0.9992105 ## 41 0.7122415 0.04143337 0.9992982 ## 43 0.7119018 0.03573791 0.9993860 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was k = 37. plot(knn_model) 5.8.4 Support vector machine model Challenge Apply a svm model, making sure you reuse the same train control object. Hint: Look at names(getModelInfo()) for all possible model names. svm_model &lt;- train(churn ~ ., churnTrain, metric = &quot;ROC&quot;, method = &quot;svmRadial&quot;, tuneLength = 10, trControl = myControl) ## maximum number of iterations reached -0.0001257902 0.0001205084maximum number of iterations reached -0.0006227097 0.0005710127maximum number of iterations reached 0.0003863967 -0.0003202759maximum number of iterations reached -0.00055456 0.0003969818maximum number of iterations reached -0.0007188688 0.0005149418maximum number of iterations reached -2.898928e-05 2.065934e-05maximum number of iterations reached -3.407462e-05 2.428845e-05maximum number of iterations reached 0.0001003576 -7.124381e-05maximum number of iterations reached 0.0001549887 -0.0001482556maximum number of iterations reached 2.486852e-05 -2.273928e-05maximum number of iterations reached 0.0001809259 -0.0001730299maximum number of iterations reached 0.0001164345 -0.000106206maximum number of iterations reached -0.0008060141 0.0007742342maximum number of iterations reached -0.000885553 0.0008157047maximum number of iterations reached -0.001607283 0.001341666maximum number of iterations reached -0.0005574151 0.0003988282maximum number of iterations reached 0.0007413495 -0.0005259731maximum number of iterations reached -0.0005681514 0.000406542maximum number of iterations reached -0.001310631 0.0009297534maximum number of iterations reached -0.001504384 0.0010636maximum number of iterations reached -0.001542939 0.001089805maximum number of iterations reached -0.001230398 0.0008740843maximum number of iterations reached 6.790484e-05 -6.498721e-05maximum number of iterations reached 0.0001186523 -0.000108221 print(svm_model) ## Support Vector Machines with Radial Basis Function Kernel ## ## 3333 samples ## 19 predictors ## 2 classes: &#39;yes&#39;, &#39;no&#39; ## ## No pre-processing ## Resampling: Bootstrapped (5 reps) ## Summary of sample sizes: 666, 667, 667, 666, 667 ## Resampling results across tuning parameters: ## ## C ROC Sens Spec ## 0.25 0.5365726 0 1.0000000 ## 0.50 0.5365465 0 1.0000000 ## 1.00 0.5307694 0 0.9992982 ## 2.00 0.5968179 0 0.9992982 ## 4.00 0.4704393 0 0.9993860 ## 8.00 0.4704644 0 0.9985088 ## 16.00 0.4081586 0 0.9988596 ## 32.00 0.4704441 0 0.9987719 ## 64.00 0.5345049 0 0.9990351 ## 128.00 0.5968091 0 0.9991228 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.007414262 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.007414262 and C = 2. plot(svm_model) 5.8.5 Naive Bayes Challenge Apply a naive Bayes model, making sure you reuse the same train control object. nb_model &lt;- train(churn ~ ., churnTrain, metric = &quot;ROC&quot;, method = &quot;naive_bayes&quot;, trControl = myControl) print(nb_model) ## Naive Bayes ## ## 3333 samples ## 19 predictors ## 2 classes: &#39;yes&#39;, &#39;no&#39; ## ## No pre-processing ## Resampling: Bootstrapped (5 reps) ## Summary of sample sizes: 666, 667, 667, 666, 667 ## Resampling results across tuning parameters: ## ## usekernel ROC Sens Spec ## FALSE 0.4189977 0.08130832 0.8746491 ## TRUE 0.8058312 0.00000000 1.0000000 ## ## Tuning parameter &#39;fL&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;adjust&#39; was held constant at a value of 1 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were fL = 0, usekernel = TRUE ## and adjust = 1. plot(nb_model) 5.8.6 Comparing models We can now use the caret::resamples function that will compare the models and pick the one with the highest AUC and lowest AUC standard deviation. model_list &lt;- list(glmmet = glm_model, rf = rf_model, knn = knn_model, svm = svm_model, nb = nb_model) resamp &lt;- resamples(model_list) resamp ## ## Call: ## resamples.default(x = model_list) ## ## Models: glmmet, rf, knn, svm, nb ## Number of resamples: 5 ## Performance metrics: ROC, Sens, Spec ## Time estimates for: everything, final model fit summary(resamp) ## ## Call: ## summary.resamples(object = resamp) ## ## Models: glmmet, rf, knn, svm, nb ## Number of resamples: 5 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## glmmet 0.7672189 0.7717145 0.7719128 0.7734827 0.7762158 0.7803516 0 ## rf 0.8968148 0.8989484 0.9015578 0.9022172 0.9033037 0.9104611 0 ## knn 0.6969638 0.7146390 0.7185091 0.7150380 0.7223889 0.7226892 0 ## svm 0.3443033 0.6507338 0.6557137 0.5968179 0.6652009 0.6681376 0 ## nb 0.7911213 0.7958588 0.8096980 0.8058312 0.8152081 0.8172700 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## glmmet 0.01033592 0.01033592 0.01813472 0.01760587 0.02331606 0.02590674 ## rf 0.49095607 0.53626943 0.58031088 0.57663038 0.61240310 0.66321244 ## knn 0.01808786 0.03359173 0.04404145 0.04867789 0.05958549 0.08808290 ## svm 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 ## nb 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 ## NA&#39;s ## glmmet 0 ## rf 0 ## knn 0 ## svm 0 ## nb 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## glmmet 0.9969298 0.9986842 0.9991228 0.9986842 0.9991228 0.9995614 0 ## rf 0.9697368 0.9846491 0.9899123 0.9857018 0.9916667 0.9925439 0 ## knn 0.9978070 0.9986842 0.9991228 0.9988596 0.9991228 0.9995614 0 ## svm 0.9964912 1.0000000 1.0000000 0.9992982 1.0000000 1.0000000 0 ## nb 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0 lattice::bwplot(resamp, metric = &quot;ROC&quot;) Figure 5.3: Comparing distributions of AUC values for various models. 5.8.7 Pre-processing The random forest appears to be the best one. This might be related to its ability to cope well with different types of input and require little pre-processing. Challenge If you haven’t done so, consider pre-processing the data prior to training for a model that didn’t perform well and assess whether pre-processing affected to modelling. svm_model1 &lt;- train(churn ~ ., churnTrain, metric = &quot;ROC&quot;, method = &quot;svmRadial&quot;, tuneLength = 10, trControl = myControl) ## maximum number of iterations reached 9.404755e-05 -9.008094e-05maximum number of iterations reached -0.0002925528 0.0002682108maximum number of iterations reached 0.0004396815 -0.0003642742maximum number of iterations reached -0.0005281173 0.0003772343maximum number of iterations reached -0.0004739693 0.0003384821maximum number of iterations reached 9.358567e-05 -6.64484e-05maximum number of iterations reached -0.001136635 0.0008138266maximum number of iterations reached 0.0004903903 -0.0003487462maximum number of iterations reached -2.445275e-05 1.742317e-05maximum number of iterations reached 1.939925e-05 -1.379596e-05maximum number of iterations reached 1.951941e-05 -1.868688e-05maximum number of iterations reached -6.562668e-05 6.023051e-05maximum number of iterations reached 0.000246633 -0.000235725maximum number of iterations reached -0.0007101679 0.0006820653maximum number of iterations reached -0.001048904 0.0009658041maximum number of iterations reached 1.151978e-05 -9.579987e-06maximum number of iterations reached 0.0007776988 -0.0005530895maximum number of iterations reached -0.0006997083 0.000500055maximum number of iterations reached -0.0006350491 0.0004541533maximum number of iterations reached -0.0005882566 0.00042082maximum number of iterations reached -0.000570176 0.0004079427maximum number of iterations reached 0.0006971342 -0.0004946581maximum number of iterations reached 0.0009399534 -0.0006685108maximum number of iterations reached 0.0003319118 -0.0003170281maximum number of iterations reached 9.233159e-05 -8.428216e-05 svm_model2 &lt;- train(churn ~ ., churnTrain[, c(2, 6:20)], metric = &quot;ROC&quot;, method = &quot;svmRadial&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;, &quot;pca&quot;), tuneLength = 10, trControl = myControl) model_list &lt;- list(svm1 = svm_model1, svm2 = svm_model2) resamp &lt;- resamples(model_list) summary(resamp) ## ## Call: ## summary.resamples(object = resamp) ## ## Models: svm1, svm2 ## Number of resamples: 5 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## svm1 0.6506942 0.6556037 0.6558074 0.6590755 0.665152 0.6681199 0 ## svm2 0.7195396 0.7258102 0.7333526 0.7317216 0.736680 0.7432256 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## svm1 0 0 0.00000000 0.00000000 0.00000000 0.00000000 0 ## svm2 0 0 0.03359173 0.02329866 0.04145078 0.04145078 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## svm1 1.0000000 1.0000000 1.0000000 1.0000000 1.000000 1.000000 0 ## svm2 0.9741228 0.9907895 0.9916667 0.9904386 0.997807 0.997807 0 bwplot(resamp, metric = &quot;ROC&quot;) 5.8.8 Predict using the best model Challenge Choose the best model using the resamples function and comparing the results and apply it to predict the churnTest labels. p &lt;- predict(rf_model, churnTest) confusionMatrix(p, churnTest$churn) ## Confusion Matrix and Statistics ## ## Reference ## Prediction yes no ## yes 165 3 ## no 59 1440 ## ## Accuracy : 0.9628 ## 95% CI : (0.9526, 0.9714) ## No Information Rate : 0.8656 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.8212 ## Mcnemar&#39;s Test P-Value : 2.848e-12 ## ## Sensitivity : 0.73661 ## Specificity : 0.99792 ## Pos Pred Value : 0.98214 ## Neg Pred Value : 0.96064 ## Prevalence : 0.13437 ## Detection Rate : 0.09898 ## Detection Prevalence : 0.10078 ## Balanced Accuracy : 0.86726 ## ## &#39;Positive&#39; Class : yes ## There are exactly 238 available methods. See http://topepo.github.io/caret/train-models-by-tag.html for details.↩ "],
["final-notes.html", "Chapter 6 Final notes 6.1 Other learning algorithms 6.2 Credit 6.3 References and further reading 6.4 Session information", " Chapter 6 Final notes 6.1 Other learning algorithms 6.1.1 Semi-supervised learning Semi-supervised learning and novelty detection 6.1.2 Deep learning in R This teching material focused on introductory material https://blog.rstudio.com/2017/09/05/keras-for-r/ 6.1.3 Model performance macro F1 scores 6.2 Credit Many parts of this course have been influenced by the DataCamp’s Machine Learning with R skill track, in particular the Machine Learning Toolbox (supervised learning chapter) and the Unsupervised Learning in R (unsupervised learning chapter) courses. The very hands-on approach has also been influenced by the Software and Data Carpentry lessons and teaching styles. 6.3 References and further reading caret: Classification and Regression Training. Max Kuhn. https://CRAN.R-project.org/package=caret. Applied predictive modeling, Max Kuhn and Kjell and caret bbok Johnson. http://appliedpredictivemodeling.com/. An Introduction to Statistical Learning (with Applications in R). Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. mlr: Machine Learning in R. Bischl B, Lang M, Kotthoff L, Schiffner J, Richter J, Studerus E, Casalicchio G and Jones Z (2016). Journal of Machine Learning Research_, 17(170), pp. 1-5. https://github.com/mlr-org/mlr. DataCamp’s Machine Learning with R skill track (requires paid access). 6.4 Session information sessionInfo() ## R version 3.4.2 Patched (2017-10-03 r73455) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.5 LTS ## ## Matrix products: default ## BLAS: /usr/lib/atlas-base/libf77blas.so.3.0 ## LAPACK: /usr/lib/lapack/liblapack.so.3.0 ## ## locale: ## [1] LC_CTYPE=en_GB.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_GB.UTF-8 LC_COLLATE=en_GB.UTF-8 ## [5] LC_MONETARY=en_GB.UTF-8 LC_MESSAGES=en_GB.UTF-8 ## [7] LC_PAPER=en_GB.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] Rtsne_0.13 MASS_7.3-47 bookdown_0.5 ## [4] naivebayes_0.9.1 kernlab_0.9-25 dplyr_0.7.4 ## [7] e1071_1.6-8 impute_1.51.0 randomForest_4.6-12 ## [10] rpart.plot_2.1.2 rpart_4.1-11 ranger_0.8.0 ## [13] caTools_1.17.1 class_7.3-14 knitr_1.17 ## [16] DT_0.2 mlbench_2.1-1 scales_0.5.0 ## [19] BiocStyle_2.5.35 glmnet_2.0-13 foreach_1.4.3 ## [22] Matrix_1.2-11 C50_0.1.0-24 caret_6.0-77 ## [25] ggplot2_2.2.1 lattice_0.20-35 ## ## loaded via a namespace (and not attached): ## [1] ddalpha_1.3.1 jsonlite_1.5 sfsmisc_1.1-1 ## [4] splines_3.4.2 prodlim_1.6.1 Formula_1.2-2 ## [7] assertthat_0.2.0 highr_0.6 stats4_3.4.2 ## [10] DRR_0.0.2 msdata_0.17.1 yaml_2.1.14 ## [13] robustbase_0.92-7 ipred_0.9-6 backports_1.1.1 ## [16] glue_1.1.1 digest_0.6.12 colorspace_1.3-2 ## [19] recipes_0.1.0 htmltools_0.3.6 plyr_1.8.4 ## [22] timeDate_3012.100 pkgconfig_2.0.1 CVST_0.2-1 ## [25] purrr_0.2.3 RANN_2.5.1 gower_0.1.2 ## [28] lava_1.5.1 tibble_1.3.4 withr_2.0.0 ## [31] nnet_7.3-12 lazyeval_0.2.0 survival_2.41-3 ## [34] magrittr_1.5 evaluate_0.10.1 nlme_3.1-131 ## [37] dimRed_0.1.0 tools_3.4.2 stringr_1.2.0 ## [40] munsell_0.4.3 bindrcpp_0.2 compiler_3.4.2 ## [43] RcppRoll_0.2.2 rlang_0.1.2 grid_3.4.2 ## [46] rstudioapi_0.7 iterators_1.0.8 htmlwidgets_0.9 ## [49] bitops_1.0-6 rmarkdown_1.6 partykit_1.1-1 ## [52] gtable_0.2.0 ModelMetrics_1.1.0 codetools_0.2-15 ## [55] reshape2_1.4.2 R6_2.2.2 lubridate_1.6.0 ## [58] bindr_0.1 rprojroot_1.2 stringi_1.1.5 ## [61] Rcpp_0.12.13 DEoptimR_1.0-8 "]
]
